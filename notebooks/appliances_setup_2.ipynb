{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc31ce4bec80711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:10:58.739820Z",
     "start_time": "2025-05-24T21:10:55.113409Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"appliances_similarity\") \\\n",
    "    .config(\"spark.local.dir\", \"/media/backup\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.default.parallelism\", \"6\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab894740b4c863",
   "metadata": {},
   "source": [
    "# Step 1: Read and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31317cd83223dfce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:04.279013Z",
     "start_time": "2025-05-24T21:10:58.757131Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "\n",
    "df = spark.read\\\n",
    "    .json(\"/media/backup/datasets/ENSA M2/meta_Appliances.json\")\\\n",
    "    .select(\"asin\", \"category\", \"description\", \"title\", \"feature\", \"brand\", \"main_cat\", \"price\", \"imageURLHighRes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3681103e5bf8d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:04.553786Z",
     "start_time": "2025-05-24T21:11:04.477900Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract, lit, when\n",
    "\n",
    "# Define the regex pattern\n",
    "pattern = r\"\\$\\d+\\.\\d{2}\"\n",
    "\n",
    "# Create a new column with the extracted price\n",
    "# regexp_extract returns an empty string if no match is found\n",
    "df_with_extracted_price = df.withColumn(\n",
    "    \"extracted_price_temp\",\n",
    "    regexp_extract(col(\"price\"), pattern, 0)\n",
    ")\n",
    "\n",
    "# Now, use when/otherwise to set the final 'cleaned_price' column\n",
    "# If 'extracted_price_temp' is not empty, use its value.\n",
    "# Otherwise (if no match was found by regexp_extract), set it to an empty string.\n",
    "# Also handle original nulls in 'price' column, setting them to empty string as well.\n",
    "df_cleaned_price = df_with_extracted_price.withColumn(\n",
    "    \"cleaned_price\",\n",
    "    when(col(\"price\").isNull(), lit(\"\")) # Handle original nulls first\n",
    "    .when(col(\"extracted_price_temp\") != lit(\"\"), col(\"extracted_price_temp\")) # If a match was found\n",
    "    .otherwise(lit(\"\")) # If no match (regexp_extract returned empty string or price was not null but not matched)\n",
    ")\n",
    "\n",
    "# Drop the temporary column\n",
    "df = df_cleaned_price.drop(\"extracted_price_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fc08277178775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:08.227366Z",
     "start_time": "2025-05-24T21:11:05.111360Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count duplicates per ASIN\n",
    "duplicates = df.groupBy(\"asin\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Show duplicates (if any)\n",
    "print(f\"Total duplicate ASINs: {duplicates.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d4ed1706f56ea57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:08.344797Z",
     "start_time": "2025-05-24T21:11:08.330639Z"
    }
   },
   "outputs": [],
   "source": [
    "df_deduplicated_by_asin = df.dropDuplicates([\"asin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9be1630c2c38697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:08.553081Z",
     "start_time": "2025-05-24T21:11:08.437324Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, coalesce, lit, concat_ws, lower, regexp_replace, row_number, collect_list\n",
    "\n",
    "# Clean nulls and convert arrays to strings\n",
    "# Convert ALL array columns to strings first\n",
    "df_clean = df.withColumn(\n",
    "    \"category\",\n",
    "    coalesce(concat_ws(\" \", col(\"category\")), lit(\"\"))  # Array → String\n",
    ").withColumn(\n",
    "    \"description\",\n",
    "    coalesce(concat_ws(\" \", col(\"description\")), lit(\"\"))  # Fix here\n",
    ").withColumn(\n",
    "    \"title\",\n",
    "    coalesce(col(\"title\"), lit(\"\"))  # Already a string\n",
    ").withColumn(\n",
    "    \"feature\",\n",
    "    coalesce(concat_ws(\" \", col(\"feature\")), lit(\"\"))  # Array → String\n",
    ").select(\n",
    "   c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ce8963017e7d1",
   "metadata": {},
   "source": [
    "# Step 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5080575794e60a42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:08.733909Z",
     "start_time": "2025-05-24T21:11:08.654522Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, trim\n",
    "\n",
    "# Combine text columns\n",
    "df_combined = df_clean.withColumn(\n",
    "    \"combined_text\",\n",
    "    concat_ws(\" \", col(\"category\"), col(\"description\"), col(\"title\"), col(\"feature\"))\n",
    ")\n",
    "\n",
    "# Enhanced text cleaning\n",
    "df_clean_text = df_combined.withColumn(\n",
    "    \"cleaned_text\",\n",
    "    trim(  # Final whitespace trim\n",
    "        regexp_replace(  # Replace multiple spaces\n",
    "            regexp_replace(  # Remove non-alphanumeric\n",
    "                regexp_replace(  # Remove HTML tags\n",
    "                    regexp_replace(  # Replace &gt;\n",
    "                        regexp_replace(  # Replace &lt;\n",
    "                            regexp_replace(  # Replace &amp;\n",
    "                                lower(col(\"combined_text\")),\n",
    "                                \"&amp;\", \"&\"\n",
    "                            ),\n",
    "                            \"&lt;\", \"<\"\n",
    "                        ),\n",
    "                        \"&gt;\", \">\"\n",
    "                    ),\n",
    "                    \"<[^>]+>\", \"\"  # Remove HTML tags\n",
    "                ),\n",
    "                \"[^a-zA-Z0-9\\\\s]\", \"\"  # Non-alphanumeric removal\n",
    "            ),\n",
    "            \"\\\\s+\", \" \"  # Multiple spaces to single\n",
    "        )\n",
    "    )\n",
    ").select(\"asin\", \"cleaned_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e9b6c3f4cbe9",
   "metadata": {},
   "source": [
    "# Step 3: TF-IDF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e63f1750c0608a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:22.134180Z",
     "start_time": "2025-05-24T21:11:08.828646Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Tokenization and stopwords removal remains the same\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
    "df_tokens = tokenizer.transform(df_clean_text)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "df_filtered = remover.transform(df_tokens)\n",
    "\n",
    "# Better term filtering using CountVectorizer\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"filtered_tokens\",\n",
    "    outputCol=\"raw_features\",\n",
    "    vocabSize=2**10,  # Match your numFeatures\n",
    "    minDF=5,         # Ignore terms in <5 docs\n",
    "    maxDF=0.9        # Ignore terms in >90% docs\n",
    ")\n",
    "cv_model = cv.fit(df_filtered)\n",
    "df_tf = cv_model.transform(df_filtered)\n",
    "\n",
    "# IDF calculation\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf_features\")\n",
    "idf_model = idf.fit(df_tf)\n",
    "df_tfidf = idf_model.transform(df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2dd9a7ce318ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:23.065062Z",
     "start_time": "2025-05-24T21:11:22.246594Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total rows before removing zero vectors: {df_tfidf.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a7ed09cb8ab414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:23.238090Z",
     "start_time": "2025-05-24T21:11:23.194778Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Check if vector has non-zero values\n",
    "has_non_zero = udf(lambda v: v.numNonzeros() > 0, BooleanType())  # Add () to call the method\n",
    "\n",
    "# Filter empty vectors\n",
    "df_tfidf = df_tfidf.filter(has_non_zero(\"tf_idf_features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5bff62d8efbca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:29.094183Z",
     "start_time": "2025-05-24T21:11:23.349281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check results\n",
    "print(f\"Total rows after filtering: {df_tfidf.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d4254806d3da2",
   "metadata": {},
   "source": [
    "# Step 4: Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a76a0e5593905c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:11:29.219767Z",
     "start_time": "2025-05-24T21:11:29.175724Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "# 1. Switch to MinHashLSH for cosine/Jaccard similarity\n",
    "mh = MinHashLSH(\n",
    "    inputCol=\"tf_idf_features\",\n",
    "    outputCol=\"hashes\",\n",
    "    numHashTables=15  # More tables = better recall\n",
    ")\n",
    "model = mh.fit(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6732a642c87d5db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:17:49.509076Z",
     "start_time": "2025-05-24T21:12:07.005871Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Full processing with memory optimizations\n",
    "df_tfidf.cache().count()  # Force cache\n",
    "\n",
    "# Process in larger chunks (20% each)\n",
    "chunks = df_tfidf.randomSplit([0.2] * 5)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    # Process against full dataset but limit output\n",
    "    similar_chunk = model.approxSimilarityJoin(\n",
    "        chunk.limit(10000),  # Safety limit\n",
    "        df_tfidf,\n",
    "        0.6,\n",
    "        distCol=\"jaccardDist\"\n",
    "    ).filter(col(\"datasetA.asin\") != col(\"datasetB.asin\"))\n",
    "\n",
    "    # Write with overwrite to save space\n",
    "    similar_chunk.write.mode(\"overwrite\").parquet(f\"/media/backup/datasets/ENSA M2/BigData/output/chunk_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b1092b0ad40b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:22:09.930985Z",
     "start_time": "2025-05-24T22:20:53.635943Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, least, greatest, row_number, collect_list\n",
    "from pyspark.sql.window import Window # Import Window\n",
    "\n",
    "# --- 1. Read Raw Data ---\n",
    "# Make sure this path correctly points to your chunked Parquet files.\n",
    "input_parquet_path = \"/media/backup/datasets/ENSA M2/BigData/output/*\"\n",
    "similar_items_raw = spark.read.parquet(input_parquet_path)\n",
    "\n",
    "print(\"Original schema of the 'similar_items_raw' DataFrame:\")\n",
    "similar_items_raw.printSchema()\n",
    "\n",
    "# --- 2. Flatten and Alias Columns ---\n",
    "# This creates top-level columns 'asin_A', 'asin_B', and 'distance'\n",
    "items_flattened = similar_items_raw.select(\n",
    "    col(\"datasetA.asin\").alias(\"asin_A\"),\n",
    "    col(\"datasetB.asin\").alias(\"asin_B\"),\n",
    "    col(\"jaccardDist\").alias(\"distance\") # Ensure this matches your actual column name\n",
    ")\n",
    "\n",
    "print(\"\\nSchema after flattening: 'items_flattened' DataFrame:\")\n",
    "items_flattened.printSchema()\n",
    "\n",
    "# --- 3. Deduplicate Pairs (including reciprocal pairs) ---\n",
    "# This ensures each unique (item1, item2) pair appears only once.\n",
    "# The result is stored in 'unique_similar_items'.\n",
    "unique_similar_items = items_flattened.withColumn(\n",
    "    \"temp_asin_A\", least(col(\"asin_A\"), col(\"asin_B\"))\n",
    ").withColumn(\n",
    "    \"temp_asin_B\", greatest(col(\"asin_A\"), col(\"asin_B\"))\n",
    ").dropDuplicates([\"temp_asin_A\", \"temp_asin_B\"]) \\\n",
    " .drop(\"temp_asin_A\", \"temp_asin_B\") # Drop temporary columns\n",
    "\n",
    "print(\"\\nSchema after deduplication: 'unique_similar_items' DataFrame:\")\n",
    "unique_similar_items.printSchema()\n",
    "\n",
    "# --- 4. Select and Rename for Ranking ---\n",
    "# NOW, use 'unique_similar_items' which has 'asin_A', 'asin_B', and 'distance'.\n",
    "# Rename them for clarity in the ranking step.\n",
    "ranked_similar = unique_similar_items.select(\n",
    "    col(\"asin_A\").alias(\"asin\"),             # Rename to 'asin' for partitioning\n",
    "    col(\"asin_B\").alias(\"similar_asin\"),     # Rename to 'similar_asin'\n",
    "    col(\"distance\").alias(\"jaccard_distance\") # Use the 'distance' alias from above\n",
    ")\n",
    "\n",
    "print(\"\\nSchema for ranking: 'ranked_similar' DataFrame:\")\n",
    "ranked_similar.printSchema()\n",
    "\n",
    "# --- 5. Define Window and Add Rank ---\n",
    "# Define the window using the new column names ('asin' and 'jaccard_distance')\n",
    "window = Window.partitionBy(\"asin\").orderBy(col(\"jaccard_distance\").asc())\n",
    "\n",
    "# Add rank and filter to get top N similar items\n",
    "ranked_similar = ranked_similar.withColumn(\"rank\", row_number().over(window)) \\\n",
    "    .filter(col(\"rank\") <= 20) # Keeping top 20 similar items\n",
    "\n",
    "# --- 6. Aggregate Results ---\n",
    "# Group by the primary ASIN and collect the list of top similar ASINs\n",
    "final_output = ranked_similar.groupBy(\"asin\") \\\n",
    "    .agg(collect_list(\"similar_asin\").alias(\"similar_items\")) # Renamed to 'similar_items' as you requested\n",
    "\n",
    "print(\"\\nSchema of the final output DataFrame:\")\n",
    "final_output.printSchema()\n",
    "\n",
    "print(\"\\nSample of final output (first 20 rows):\")\n",
    "final_output.show(20, truncate=False)\n",
    "\n",
    "# --- 7. Visualize Matches for Sample Product (Optional) ---\n",
    "# Ensure 'display' is available if you're in a notebook environment like Databricks or Colab.\n",
    "# If not, use .show() or .toPandas() and then print()\n",
    "try:\n",
    "    sample_asins_df = ranked_similar.limit(25).toPandas()\n",
    "    print(\"\\nTop Matches Example (for ranking validation):\")\n",
    "    from IPython.display import display # For display in notebooks\n",
    "    display(sample_asins_df[['asin', 'similar_asin', 'jaccard_distance', 'rank']])\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not display sample: {e}. If not in a notebook, use .show() or .toPandas().\")\n",
    "    ranked_similar.show(25, truncate=False)\n",
    "\n",
    "\n",
    "print(\"\\nFinal DataFrame 'final_output' has the structure {'asin': string, 'similar_items': list<string>}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ced51d2ae4390c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:40:08.114089Z",
     "start_time": "2025-05-24T22:39:38.295805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'final_output' is your DataFrame to export\n",
    "output_path_parquet = \"/media/backup/datasets/ENSA M2/BigData/project/similar_products_parquets\"\n",
    "\n",
    "final_output.write.mode(\"overwrite\").parquet(output_path_parquet)\n",
    "\n",
    "print(f\"DataFrame successfully exported to Parquet at: {output_path_parquet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc654eb64d1385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:40:12.979917Z",
     "start_time": "2025-05-24T22:40:11.302653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save to Parquet\n",
    "output_path_parquet = \"/media/backup/datasets/ENSA M2/BigData/project/products_metadata_parquets\" # Or \"s3a://bucket/path/...\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path_parquet)\n",
    "print(f\"Filtered DataFrame exported to Parquet at: {output_path_parquet}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
